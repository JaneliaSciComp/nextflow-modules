# yaml-language-server: $schema=https://raw.githubusercontent.com/nf-core/modules/master/subworkflows/yaml-schema.json
name: spark_start
description: |
  Starts Spark processing either by spinning up a cluster or setting up
  variables so that processing can run locally as individual jobs
keywords:
  - spark
  - bigdata
  - infrastructure
components: []
input:
  - ch_meta:
      type: tuple
      description: |
        Channel of tuples containing a meta map and a list of data paths
        Structure: [ val(meta), [path(data_paths)] ]
  - config:
      type: map
      description: additional spark configuration
  - spark_cluster:
      type: boolean
      description: Whether or not to spin up a Spark cluster
  - working_dir:
      type: path
      description: Path shared by workers for logging and jar distribution
  - spark_workers:
      type: integer
      description: Number of workers in the cluster
  - min_workers:
      type: integer
      description: Minimum number of spark workers that must be available on the spark cluster
  - spark_worker_cpus:
      type: integer
      description: Number of CPUs per Spark worker
  - spark_executor_cpus:
      type: integer
      description: Number of CPUs for a spark executor
  - spark_executor_mem_gb:
      type: integer
      description: Memory resource in GB allocated for a spark executor
  - spark_executor_overhead_mem_gb:
      type: integer
      description: Memory overhead for a spark executor
  - spark_driver_cpus:
      type: integer
      description: Number of CPUs for the Spark driver
  - spark_driver_mem_gb:
      type: integer
      description: Number of GB of memory for the Spark driver
  - spark_gb_per_core:
      type: integer
      description: Number of GB of memory per worker core

output:
  - spark_context:
      type: tuple
      description: |
        The tuple from input ch_meta with the spark_context map appended.
        Structure: [ val(meta), val(spark_context) ]

authors:
  - "@krokicki"
  - "@cgoina"